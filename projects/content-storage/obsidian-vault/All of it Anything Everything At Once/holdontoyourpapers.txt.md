
# holdontoyourpapers.txt
4:03 PM]Glitch: https://arxiv.org/abs/2303.03103
arXiv.org
Towards Zero-Shot Functional Compositionality of Language Models
Large Pre-trained Language Models (PLM) have become the most desirable
starting point in the field of NLP, as they have become remarkably good at
solving many individual tasks. Despite such...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2303.03004
arXiv.org
xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code...
The ability to solve problems is a hallmark of intelligence and has been an
enduring goal in AI. AI systems that can create programs as solutions to
problems or assist developers in writing...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2302.04761
arXiv.org
Toolformer: Language Models Can Teach Themselves to Use Tools
Language models (LMs) exhibit remarkable abilities to solve new tasks from
just a few examples or textual instructions, especially at scale. They also,
paradoxically, struggle with basic...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2303.00001
arXiv.org
Reward Design with Language Models
Reward design in reinforcement learning (RL) is challenging since specifying
human notions of desired behavior may be difficult via reward functions or
require many expert demonstrations. Can we...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2302.06706
arXiv.org
On the Planning Abilities of Large Language Models (A Critical...
Intrigued by the claims of emergent reasoning capabilities in LLMs trained on
general web corpora, in this paper, we set out to investigate their planning
capabilities. We aim to evaluate (1) how...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2207.05987
arXiv.org
DocPrompting: Generating Code by Retrieving the Docs
Publicly available source-code libraries are continuously growing and
changing. This makes it impossible for models of code to keep current with all
available APIs by simply training these models...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2302.14045
arXiv.org
Language Is Not All You Need: Aligning Perception with Language Models
A big convergence of language, multimodal perception, action, and world
modeling is a key step toward artificial general intelligence. In this work, we
introduce Kosmos-1, a Multimodal Large...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2302.11466
arXiv.org
Advancements in Federated Learning: Models, Methods, and Privacy
Federated learning (FL) is a promising technique for addressing the rising
privacy and security issues. Its main ingredient is to cooperatively learn the
model among the distributed clients...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2212.09689
arXiv.org
Unnatural Instructions: Tuning Language Models with (Almost) No Hum...
Instruction tuning enables pretrained language models to perform new tasks
from inference-time natural language descriptions. These approaches rely on
vast amounts of human supervision in the form...
Image
[4:03 PM]Glitch: https://arxiv.org/abs/2210.05359
arXiv.org
Mind's Eye: Grounded Language Model Reasoning through Simulation
Successful and effective communication between humans and AI relies on a
shared experience of the world. By training solely on written text, current
language models (LMs) miss the grounded...
Image
[5:13 PM]Glitch: https://arxiv.org/abs/2303.05398
arXiv.org
MathPrompter: Mathematical Reasoning using Large Language Models
Large Language Models (LLMs) have limited performance when solving arithmetic
reasoning tasks and often provide incorrect answers. Unlike natural language
understanding, math problems typically...
Image
[5:13 PM]Glitch: https://arxiv.org/abs/2303.05510
arXiv.org
Planning with Large Language Models for Code Generation
Existing large language model-based code generation pipelines typically use
beam search or sampling algorithms during the decoding process. Although the
programs they generate achieve high token-matching-based scores, they often
fail to compile or generate incorrect outputs. The main reason is that
conventional Transformer decoding algorithms ma...
Image
[5:13 PM]Glitch: https://ai.googleblog.com/2022/11/better-language-models-without-massive.html?m=1
Better Language Models Without Massive Compute
Better Language Models Without Massive Compute
[5:17 PM]Glitch: https://www.nature.com/articles/s41562-022-01516-2
Nature
Evidence of a predictive coding hierarchy in the human brain listen...
Nature Human Behaviour - Current machine learning language algorithms make adjacent word-level predictions. In this work, Caucheteux et al. show that the human brain probably uses long-range and...
[5:29 PM]Glitch: https://arxiv.org/abs/2302.06675
https://arxiv.org/abs/2302.07080
https://arxiv.org/abs/2212.14034
https://arxiv.org/abs/2302.00923
arXiv.org
Symbolic Discovery of Optimization Algorithms
We present a method to formulate algorithm discovery as program search, and
apply it to discover optimization algorithms for deep neural network training.
We leverage efficient search techniques to explore an infinite and sparse
program space. To bridge the large generalization gap between proxy and target
tasks, we also introduce program select...
Image
arXiv.org
The Programmer's Assistant: Conversational Interaction with a Large...
Large language models (LLMs) have recently been applied in software
engineering to perform tasks such as translating code between programming
languages, generating code from natural language, and autocompleting code as it
is being written. When used within development tools, these systems typically
treat each model invocation independently from ...
Image
arXiv.org
Cramming: Training a Language Model on a Single GPU in One Day
Recent trends in language modeling have focused on increasing performance
through scaling, and have resulted in an environment where training language
models is out of reach for most researchers and practitioners. While most in
the community are asking how to push the limits of extreme computation, we ask
the opposite question: How far can we ge...
Image
arXiv.org
Multimodal Chain-of-Thought Reasoning in Language Models
Large language models (LLMs) have shown impressive performance on complex
reasoning by leveraging chain-of-thought (CoT) prompting to generate
intermediate reasoning chains as the rationale to infer the answer. However,
existing CoT studies have focused on the language modality. We propose
Multimodal-CoT that incorporates language (text) and vis...
Image
[5:30 PM]Glitch: https://arxiv.org/abs/2302.08399
arXiv.org
Large Language Models Fail on Trivial Alterations to Theory-of-Mind...
Intuitive psychology is a pillar of common-sense reasoning. The replication
of this reasoning in machine intelligence is an important stepping-stone on the
way to human-like artificial intelligence. Several recent tasks and benchmarks
for examining this reasoning in Large-Large Models have focused in particular
on belief attribution in Theory-of...
Image
[5:40 PM]Glitch: https://arxiv.org/abs/2303.05511
arXiv.org
Scaling up GANs for Text-to-Image Synthesis
The recent success of text-to-image synthesis has taken the world by storm
and captured the general public's imagination. From a technical standpoint, it
also marked a drastic change in the favored architecture to design generative
image models. GANs used to be the de facto choice, with techniques like
StyleGAN. With DALL-E 2, auto-regressive an...
Image
[5:41 PM]Glitch: https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1
Papers with Code - LLaMA: Open and Efficient Foundation Language Mo...
üèÜ SOTA for Question Answering on PIQA (Accuracy metric)
Papers with Code - LLaMA: Open and Efficient Foundation Language Mo...
Glitch
 pinned 
a message
 to this channel. See all 
pinned messages
.
 ‚Äî 03/11/2023 5:41 PM
Glitch
 pinned 
a message
 to this channel. See all 
pinned messages
.
 ‚Äî 03/11/2023 9:23 PM
[6:10 PM]Glitch: https://arxiv.org/abs/2201.08808 
arXiv.org
Conversational Information Seeking
Conversational information seeking (CIS) is concerned with a sequence of
interactions between one or more users and an information system. Interactions
in CIS are primarily based on natural language dialogue, while they may include
other types of interactions, such as click, touch, and body gestures. This
monograph provides a thorough overview o...
Image
[6:11 PM]Glitch: https://link.springer.com/chapter/10.1007/978-3-642-32090-3_21
SpringerLink
An Engineering Approach to Conversational Informatics
Conversational informatics is a field of research that focuses on investigating human behaviors and designing artifacts that can interact with people in a conversational fashion. The field draws on a foundation provided by artificial intelligence, natural language...
Image
[6:37 PM]Glitch: this would be the published textbook - I have a physical copy: might transcribe it for reference.
https://onlinelibrary.wiley.com/doi/book/10.1002/9780470512470
[7:02 PM]Glitch: https://arxiv.org/abs/2301.04589
arXiv.org
Memory Augmented Large Language Models are Computationally Universal
We show that transformer-based large language models are computationally
universal when augmented with an external memory. Any deterministic language
model that conditions on strings of bounded length is equivalent to a finite
automaton, hence computationally limited. However, augmenting such models with
a read-write memory creates the possibili...
Image
[11:11 AM]Glitch: "Prompting Large Language Models
With the Socratic Method"
https://arxiv.org/pdf/2303.08769.pdf 
[3:47 PM]Glitch: 'Cetacean Translation Initiative: a roadmap to deciphering the
communication of sperm whales'
https://arxiv.org/ftp/arxiv/papers/2104/2104.08614.pdf
[3:48 PM]Glitch: 'Sparks of Artificial General Intelligence:
Early experiments with GPT-4'
https://arxiv.org/pdf/2303.12712.pdf
[4:56 PM]Glitch: https://github.com/daveshap/Functional_Sentience/blob/main/transcript.md
GitHub
Functional_Sentience/transcript.md at main ¬∑ daveshap/Functional_Se...
Paper on Functional (vs Philosophical) Sentience. Contribute to daveshap/Functional_Sentience development by creating an account on GitHub.
Functional_Sentience/transcript.md at main ¬∑ daveshap/Functional_Se...
[10:15 AM]Glitch: https://aiindex.stanford.edu/report/
Artificial Intelligence Index
digitalavenues
AI Index Report 2023
[11:39 PM]Glitch: https://arxiv.org/abs/2212.01354
arXiv.org
Designing Ecosystems of Intelligence from First Principles
This white paper lays out a vision of research and development in the field
of artificial intelligence for the next decade (and beyond). Its denouement is
a cyber-physical ecosystem of natural and synthetic sense-making, in which
humans are integral participants$\unicode{x2014}$what we call ''shared
intelligence''. This vision is premised on act...
Image
[1:20 AM]Glitch: https://arxiv.org/abs/2004.09473
arXiv.org
Attention Routing: track-assignment detailed routing using attentio...
In the physical design of integrated circuits, global and detailed routing
are critical stages involving the determination of the interconnected paths of
each net on a circuit while satisfying the design constraints. Existing actual
routers as well as routability predictors either have to resort to expensive
approaches that lead to high computat...
Image
[12:47 PM]Glitch: https://www.biorxiv.org/content/10.1101/2022.11.18.517004v2.full.pdf
[12:47 PM]Glitch: "experiment where they were able to reconstruct a visual image from an fMRI scan using Stable Diffusion"
[6:12 PM]Glitch: @Teradimer
[6:13 PM]Teradimer: mhm
[2:42 PM]Glitch: https://arxiv.org/abs/2303.11366
arXiv.org
Reflexion: an autonomous agent with dynamic memory and self-reflection
Recent advancements in decision-making large language model (LLM) agents have
demonstrated impressive performance across various benchmarks. However, these
state-of-the-art approaches typically necessitate internal model fine-tuning,
external model fine-tuning, or policy optimization over a defined state space.
Implementing these methods can pro...
Image
[8:12 PM]Glitch: https://arxiv.org/abs/2210.03629#google
arXiv.org
ReAct: Synergizing Reasoning and Acting in Language Models
While large language models (LLMs) have demonstrated impressive capabilities
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.
action plan generation) have primarily been studied as separate topics. In this
paper, we explore the use of LLMs to ...
Image
[11:25 PM]Glitch: https://arxiv.org/abs/2212.14704
arXiv.org
Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Te...
Recent CLIP-guided 3D optimization methods, such as DreamFields and
PureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D
synthesis. However, due to scratch training and random initialization without
prior knowledge, these methods often fail to generate accurate and faithful 3D
structures that conform to the input text. In this ...
Image
[11:25 PM]Glitch: https://arxiv.org/abs/2211.10440
arXiv.org
Magic3D: High-Resolution Text-to-3D Content Creation
DreamFusion has recently demonstrated the utility of a pre-trained
text-to-image diffusion model to optimize Neural Radiance Fields (NeRF),
achieving remarkable text-to-3D synthesis results. However, the method has two
inherent limitations: (a) extremely slow optimization of NeRF and (b)
low-resolution image space supervision on NeRF, leading to...
