{
  "title": "script_01",
  "content": "# script_01\n\nContent-Type: text/x-zim-wiki\nWiki-Format: zim 0.6\nCreation-Date: 2024-07-23T15:42:35-07:00\n\n====== script 01 ======\nimport abc\r\nimport os\r\nimport openai\r\nimport yaml\r\nimport sys\r\nfrom dotenv import load_dotenv\r\n\r\n# Load environment variables from .env file\r\nload_dotenv()\r\n\r\nclass Singleton(abc.ABCMeta, type):\r\n\t\"\"\"\r\n\tSingleton metaclass for ensuring only one instance of a class.\r\n\t\"\"\"\r\n\r\n\t_instances = {}\r\n\r\n\tdef __call__(cls, *args, **kwargs):\r\n\t\t\"\"\"Call method for the singleton metaclass.\"\"\"\r\n\t\tif cls not in cls._instances:\r\n\t\t\tcls._instances[cls] = super(\r\n\t\t\t\tSingleton, cls).__call__(\r\n\t\t\t\t*args, **kwargs)\r\n\t\treturn cls._instances[cls]\r\n\r\n\r\nclass AbstractSingleton(abc.ABC, metaclass=Singleton):\r\n\tpass\r\n\r\n\r\nclass Config(metaclass=Singleton):\r\n\t\"\"\"\r\n\tConfiguration class to store the state of bools for different scripts access.\r\n\t\"\"\"\r\n\r\n\tdef __init__(self):\r\n\t\t\"\"\"Initialize the Config class\"\"\"\r\n\t\tself.debug_mode = False\r\n\t\tself.continuous_mode = False\r\n\t\tself.continuous_limit = 0\r\n\t\tself.speak_mode = False\r\n\r\n\t\tself.fast_llm_model = os.getenv(\"FAST_LLM_MODEL\", \"gpt-3.5-turbo\")\r\n\t\tself.smart_llm_model = os.getenv(\"SMART_LLM_MODEL\", \"gpt-4\")\r\n\t\tself.fast_token_limit = int(os.getenv(\"FAST_TOKEN_LIMIT\", 4000))\r\n\t\tself.smart_token_limit = int(os.getenv(\"SMART_TOKEN_LIMIT\", 8000))\r\n\r\n\t\t#print(self.openai_api_key)\r\n\t\t\r\n\t\tself.openai_api_key = os.getenv(\"OPEN_API_KEY\")\r\n\t\tself.temperature = float(os.getenv(\"TEMPERATURE\", \"1\"))\r\n\t\tself.use_azure = os.getenv(\"USE_AZURE\") == 'True'\r\n\t\tself.execute_local_commands = os.getenv('EXECUTE_LOCAL_COMMANDS', 'False') == 'True'\r\n\r\n\t\tif self.use_azure:\r\n\t\t\tself.load_azure_config()\r\n\t\t\topenai.api_type = self.openai_api_type\r\n\t\t\topenai.api_base = self.openai_api_base\r\n\t\t\topenai.api_version = self.openai_api_version\r\n\r\n\t\tself.elevenlabs_api_key = os.getenv(\"ELEVENLABS_API_KEY\")\r\n\t\tself.elevenlabs_voice_1_id = os.getenv(\"ErXwobaYiN019PkySvjV\")\r\n\t\tself.elevenlabs_voice_2_id = os.getenv(\"EXAVITQu4vr4xnSDxMaL\")\r\n\r\n\t\tself.use_mac_os_tts = False\r\n\t\tself.use_mac_os_tts = os.getenv(\"USE_MAC_OS_TTS\")\r\n\r\n\t\tself.google_api_key = os.getenv(\"GOOGLE_API_KEY\")\r\n\t\tself.custom_search_engine_id = os.getenv(\"CUSTOM_SEARCH_ENGINE_ID\")\r\n\r\n\t\tself.pinecone_api_key = os.getenv(\"3b0a61d7-f207-4c53-96db-d93794c4270c\")\r\n\t\tself.pinecone_region = os.getenv(\"PINECONE_API_KEY\")\r\n\r\n\t\tself.image_provider = os.getenv(\"IMAGE_PROVIDER\")\r\n\t\tself.huggingface_api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\r\n\r\n\t\t# User agent headers to use when browsing web\r\n\t\t# Some websites might just completely deny request with an error code if no user agent was found.\r\n\t\tself.user_agent_header = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\r\n\t\tself.redis_host = os.getenv(\"REDIS_HOST\", \"localhost\")\r\n\t\tself.redis\r\n\t\tself.user_agent_header = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\r\n\t\tself.redis_host = os.getenv(\"REDIS_HOST\", \"localhost\")\r\n\t\tself.redis_port = os.getenv(\"REDIS_PORT\", \"6379\")\r\n\t\tself.redis_password = os.getenv(\"REDIS_PASSWORD\", \"\")\r\n\t\tself.wipe_redis_on_start = os.getenv(\"WIPE_REDIS_ON_START\", \"True\") == 'True'\r\n\t\tself.memory_index = os.getenv(\"MEMORY_INDEX\", 'auto-gpt')\r\n\t# Note that indexes must be created on db 0 in redis, this is not configurable.\r\n\r\n\tself.memory_backend = os.getenv(\"MEMORY_BACKEND\", 'local')\r\n\t# Initialize the OpenAI API client\r\n\topenai.api_key = self.openai_api_key\r\n\r\n\t# Set the number of requests per minute for the OpenAI API\r\n\topenai.api_requestor.requests_per_minute = int(os.getenv('OPENAI_REQUESTS_PER_MINUTE', '120'))\r\n\r\n\t# Set the backend for the LLM\r\n\tos.environ[\"LARGE_LM_BACKEND\"] = os.getenv(\"LARGE_LM_BACKEND\", \"gpt\")\r\n\t# Set the number of workers for the LLM\r\n\tos.environ[\"LARGE_LM_NUM_WORKERS\"] = os.getenv(\"LARGE_LM_NUM_WORKERS\", \"1\")\r\n\r\n\t# Set the path for the LLM models\r\n\tos.environ[\"LARGE_LM_PATH\"] = os.getenv(\"LARGE_LM_PATH\", \"llm-models\")\r\n\r\n\t# Set the cache size for the LLM\r\n\tos.environ[\"LARGE_LM_CACHE_SIZE\"] = os.getenv(\"LARGE_LM_CACHE_SIZE\", \"2000\")\r\n\r\n\t# Set the maximum length for the LLM\r\n\tos.environ[\"LARGE_LM_MAX_LEN\"] = os.getenv(\"LARGE_LM_MAX_LEN\", \"2048\")\r\n\r\n\t# Set the number of retries for HTTP requests\r\n\tos.environ[\"HTTP_RETRY_COUNT\"] = os.getenv(\"HTTP_RETRY_COUNT\", \"3\")\r\n\r\n\t# Set the backoff factor for HTTP retries\r\n\tos.environ[\"HTTP_RETRY_BACKOFF_FACTOR\"] = os.getenv(\"HTTP_RETRY_BACKOFF_FACTOR\", \"0.5\")\r\n\r\n\t# Set the maximum number of retries for HTTP requests\r\n\tos.environ[\"HTTP_RETRY_MAX_RETRIES\"] = os.getenv(\"HTTP_RETRY_MAX_RETRIES\", \"5\")\r\n\r\n\t# Set the timeout for HTTP requests\r\n\tos.environ[\"HTTP_REQUEST_TIMEOUT\"] = os.getenv(\"HTTP_REQUEST_TIMEOUT\", \"10\")\r\n\r\n\t# Set the timeout for HTTP connections\r\n\tos.environ[\"HTTP_CONNECTION_TIMEOUT\"] = os.getenv(\"HTTP_CONNECTION_TIMEOUT\", \"10\")\r\n\r\n\t# Set the retry status codes for HTTP retries\r\n\tos.environ[\"HTTP_RETRY_STATUS_CODES\"] = os.getenv(\"HTTP_RETRY_STATUS_CODES\", \"500,502,503,504,522,524,408\")\r\n\r\n\t# Set the logging level\r\n\tos.environ[\"LOG_LEVEL\"] = os.getenv(\"LOG_LEVEL\", \"INFO\")\r\n\r\n\t# Set the logging format\r\n\tos.environ[\"LOG_FORMAT\"] = os.getenv(\"LOG_FORMAT\", \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\")\r\n\r\n\tdef update_from_env(self):\r\n\t\t\"\"\"\r\n\t\tUpdates the configuration from environment variables.\r\n\t\t\"\"\"\r\n\t\tself.debug_mode = os.getenv(\"DEBUG_MODE\", \"False\") == 'True'\r\n\t\tself.continuous_mode = os.getenv(\"CONTINUOUS_MODE\", \"False\") == 'True'\r\n\t\tself.continuous_limit = int(os.getenv(\"CONTINUOUS_LIMIT\", \"0\"))\r\n\t\tself.memory_backend = os.getenv(\"MEMORY_BACKEND\", 'local')\n\n\n## Metadata\n- **Source**: All of it Anything Everything At Once/script_01.txt\n- **Type**: document\n- **Tags**: document, All_of_it_Anything_Everything_At_Once, quest, bug\n- **Imported**: 2025-05-05T16:25:37.194Z",
  "source": {
    "type": "notebook",
    "path": "All of it Anything Everything At Once/script_01.txt",
    "importedAt": "2025-05-05T16:25:37.194Z"
  },
  "created": "2025-05-05T16:25:37.194Z",
  "modified": "2025-05-05T16:25:37.194Z",
  "tags": [
    "document",
    "All_of_it_Anything_Everything_At_Once",
    "quest",
    "bug"
  ]
}