{
  "title": "old_mic_vad_streaming_txt",
  "content": "# old_mic_vad_streaming.txt\n\nContent-Type: text/x-zim-wiki\nWiki-Format: zim 0.6\nCreation-Date: 2024-07-23T15:26:48-07:00\n\n====== old mic vad streaming.txt ======\nimport time, logging\nfrom datetime import datetime\nimport threading, collections, queue, os, os.path\nimport deepspeech\nimport numpy as np\nimport pyaudio\nimport wave\nimport webrtcvad\nfrom halo import Halo\nfrom scipy import signal\n\nimport nltk # Add this at the start of your file\nnltk.download('punkt') # Add this at the start of your file\n\nlogging.basicConfig(level=20)\n\nclass Audio(object):\n\t\"\"\"Streams raw audio from microphone. Data is received in a separate thread, and stored in a buffer, to be read from.\"\"\"\n\n\tFORMAT = pyaudio.paInt16\n\t# Network/VAD rate-space\n\tRATE_PROCESS = 16000\n\tCHANNELS = 1\n\tBLOCKS_PER_SECOND = 50\n\n\tdef __init__(self, callback=None, device=None, input_rate=RATE_PROCESS, file=None):\n\t\tdef proxy_callback(in_data, frame_count, time_info, status):\n\t\t\t#pylint: disable=unused-argument\n\t\t\tif self.chunk is not None:\n\t\t\t\tin_data = self.wf.readframes(self.chunk)\n\t\t\tcallback(in_data)\n\t\t\treturn (None, pyaudio.paContinue)\n\t\tif callback is None: callback = lambda in_data: self.buffer_queue.put(in_data)\n\t\tself.buffer_queue = queue.Queue()\n\t\tself.device = device\n\t\tself.input_rate = input_rate\n\t\tself.sample_rate = self.RATE_PROCESS\n\t\tself.block_size = int(self.RATE_PROCESS / float(self.BLOCKS_PER_SECOND))\n\t\tself.block_size_input = int(self.input_rate / float(self.BLOCKS_PER_SECOND))\n\t\tself.pa = pyaudio.PyAudio()\n\n\t\tkwargs = {\n\t\t\t'format': self.FORMAT,\n\t\t\t'channels': self.CHANNELS,\n\t\t\t'rate': self.input_rate,\n\t\t\t'input': True,\n\t\t\t'frames_per_buffer': self.block_size_input,\n\t\t\t'stream_callback': proxy_callback,\n\t\t}\n\n\t\tself.chunk = None\n\t\t# if not default device\n\t\tif self.device:\n\t\t\tkwargs['input_device_index'] = self.device\n\t\telif file is not None:\n\t\t\tself.chunk = 320\n\t\t\tself.wf = wave.open(file, 'rb')\n\n\t\tself.stream = self.pa.open(**kwargs)\n\t\tself.stream.start_stream()\n\n\tdef resample(self, data, input_rate):\n\t\t\"\"\"\n\t\tMicrophone may not support our native processing sampling rate, so\n\t\tresample from input_rate to RATE_PROCESS here for webrtcvad and\n\t\tdeepspeech\n\n\t\tArgs:\n\t\t\tdata (binary): Input audio stream\n\t\t\tinput_rate (int): Input audio rate to resample from\n\t\t\"\"\"\n\t\tdata16 = np.fromstring(string=data, dtype=np.int16)\n\t\tresample_size = int(len(data16) / self.input_rate * self.RATE_PROCESS)\n\t\tresample = signal.resample(data16, resample_size)\n\t\tresample16 = np.array(resample, dtype=np.int16)\n\t\treturn resample16.tostring()\n\n\tdef read_resampled(self):\n\t\t\"\"\"Return a block of audio data resampled to 16000hz, blocking if necessary.\"\"\"\n\t\treturn self.resample(data=self.buffer_queue.get(),\n\t\t\t\t\t\t\t input_rate=self.input_rate)\n\n\tdef read(self):\n\t\t\"\"\"Return a block of audio data, blocking if necessary.\"\"\"\n\t\treturn self.buffer_queue.get()\n\n\tdef destroy(self):\n\t\tself.stream.stop_stream()\n\t\tself.stream.close()\n\t\tself.pa.terminate()\n\n\tframe_duration_ms = property(lambda self: 1000 * self.block_size // self.sample_rate)\n\n\tdef write_wav(self, filename, data):\n\t\tlogging.info(\"write wav %s\", filename)\n\t\twf = wave.open(filename, 'wb')\n\t\twf.setnchannels(self.CHANNELS)\n\t\t# wf.setsampwidth(self.pa.get_sample_size(FORMAT))\n\t\tassert self.FORMAT == pyaudio.paInt16\n\t\twf.setsampwidth(2)\n\t\twf.setframerate(self.sample_rate)\n\t\twf.writeframes(data)\n\t\twf.close()\n\n\nclass VADAudio(Audio):\n\t\"\"\"Filter & segment audio with voice activity detection.\"\"\"\n\n\tdef __init__(self, aggressiveness=3, device=None, input_rate=None, file=None):\n\t\tsuper().__init__(device=device, input_rate=input_rate, file=file)\n\t\tself.vad = webrtcvad.Vad(aggressiveness)\n\n\tdef frame_generator(self):\n\t\t\"\"\"Generator that yields all audio frames from microphone.\"\"\"\n\t\tif self.input_rate == self.RATE_PROCESS:\n\t\t\twhile True:\n\t\t\t\tyield self.read()\n\t\telse:\n\t\t\twhile True:\n\t\t\t\tyield self.read_resampled()\n\n\tdef vad_collector(self, padding_ms=300, ratio=0.75, frames=None):\n\t\t\"\"\"Generator that yields series of consecutive audio frames comprising each utterence, separated by yielding a single None.\n\t\t\tDetermines voice activity by ratio of frames in padding_ms. Uses a buffer to include padding_ms prior to being triggered.\n\t\t\tExample: (frame, ..., frame, None, frame, ..., frame, None, ...)\n\t\t\t\t\t  |---utterence---|        |---utterence---|\n\t\t\"\"\"\n\t\tif frames is None: frames = self.frame_generator()\n\t\tnum_padding_frames = padding_ms // self.frame_duration_ms\n\t\tring_buffer = collections.deque(maxlen=num_padding_frames)\n\t\ttriggered = False\n\n\t\tfor frame in frames:\n\t\t\tif len(frame) < 640:\n\t\t\t\treturn\n\n\t\t\tis_speech = self.vad.is_speech(frame, self.sample_rate)\n\n\t\t\tif not triggered:\n\t\t\t\tring_buffer.append((frame, is_speech))\n\t\t\t\tnum_voiced = len([f for f, speech in ring_buffer if speech])\n\t\t\t\tif num_voiced > ratio * ring_buffer.maxlen:\n\t\t\t\t\ttriggered = True\n\t\t\t\t\tfor f, s in ring_buffer:\n\t\t\t\t\t\tyield f\n\t\t\t\t\tring_buffer.clear()\n\n\t\t\telse:\n\t\t\t\tyield frame\n\t\t\t\tring_buffer.append((frame, is_speech))\n\t\t\t\tnum_unvoiced = len([f for f, speech in ring_buffer if not speech])\n\t\t\t\tif num_unvoiced > ratio * ring_buffer.maxlen:\n\t\t\t\t\ttriggered = False\n\t\t\t\t\tyield None\n\t\t\t\t\tring_buffer.clear()\n\ndef main(ARGS):\n\t# Load DeepSpeech model\n\tif os.path.isdir(ARGS.model):\n\t\tmodel_dir = ARGS.model\n\t\tARGS.model = os.path.join(model_dir, 'output_graph.pb')\n\t\tARGS.scorer = os.path.join(model_dir, ARGS.scorer)\n\n\tprint('Initializing model...')\n\tlogging.info(\"ARGS.model: %s\", ARGS.model)\n\tmodel = deepspeech.Model(ARGS.model)\n\tif ARGS.scorer:\n\t\tlogging.info(\"ARGS.scorer: %s\", ARGS.scorer)\n\t\tmodel.enableExternalScorer(ARGS.scorer)\n\n\t# Start audio with VAD\n\tvad_audio = VADAudio(aggressiveness=ARGS.vad_aggressiveness,\n\t\t\t\t\t\t device=ARGS.device,\n\t\t\t\t\t\t input_rate=ARGS.rate,\n\t\t\t\t\t\t file=ARGS.file)\n\tprint(\"Listening (ctrl-C to exit)...\")\n\tframes = vad_audio.vad_collector()\n\n\t# Stream from microphone to DeepSpeech using VAD\n\tspinner = None\n\tif not ARGS.nospinner:\n\t\tspinner = Halo(spinner='line')\n\tstream_context = model.createStream()\n\twav_data = bytearray()\n\tfor frame in frames:\n\t\tif frame is not None:\n\t\t\tif spinner: spinner.start()\n\t\t\tlogging.debug(\"streaming frame\")\n\t\t\tstream_context.feedAudioContent(np.frombuffer(frame, np.int16))\n\t\t\tif ARGS.savewav: wav_data.extend(frame)\n\t\telse:\n\t\t\tif spinner: spinner.stop()\n\t\t\tlogging.debug(\"end utterence\")\n\t\t\tif ARGS.savewav:\n\t\t\t\tvad_audio.write_wav(os.path.join(ARGS.savewav, datetime.now().strftime(\"savewav_%Y-%m-%d_%H-%M-%S_%f.wav\")), wav_data)\n\t\t\t\twav_data = bytearray()\n\t\t\ttext = stream_context.finishStream()\n\t\t\tprint(\"Recognized: %s\" % text)\n\t\t\tstream_context = model.createStream()\n\nif __name__ == '__main__':\n\tDEFAULT_SAMPLE_RATE = 16000\n\n\timport argparse\n\tparser = argparse.ArgumentParser(description=\"Stream from microphone to DeepSpeech using VAD\")\n\n\tparser.add_argument('-v', '--vad_aggressiveness', type=int, default=3,\n\t\t\t\t\t\thelp=\"Set aggressiveness of VAD: an integer between 0 and 3, 0 being the least aggressive about filtering out non-speech, 3 the most aggressive. Default: 3\")\n\tparser.add_argument('--nospinner', action='store_true',\n\t\t\t\t\t\thelp=\"Disable spinner\")\n\tparser.add_argument('-w', '--savewav',\n\t\t\t\t\t\thelp=\"Save .wav files of utterences to given directory\")\n\tparser.add_argument('-f', '--file',\n\t\t\t\t\t\thelp=\"Read from .wav file instead of microphone\")\n\n\tparser.add_argument('-m', '--model', required=True,\n\t\t\t\t\t\thelp=\"Path to the model (protocol buffer binary file, or entire directory containing all standard-named files for model)\")\n\tparser.add_argument('-s', '--scorer',\n\t\t\t\t\t\thelp=\"Path to the external scorer file.\")\n\tparser.add_argument('-d', '--device', type=int, default=None,\n\t\t\t\t\t\thelp=\"Device input index (Int) as listed by pyaudio.PyAudio.get_device_info_by_index(). If not provided, falls back to PyAudio.get_default_device().\")\n\tparser.add_argument('-r', '--rate', type=int, default=DEFAULT_SAMPLE_RATE,\n\t\t\t\t\t\thelp=f\"Input device sample rate. Default: {DEFAULT_SAMPLE_RATE}. Your device may require 44100.\")\n\n\tARGS = parser.parse_args()\n\tif ARGS.savewav: os.makedirs(ARGS.savewav, exist_ok=True)\n\tmain(ARGS)\n\n\n## Metadata\n- **Source**: All of it Anything Everything At Once/old_mic_vad_streaming.txt.txt\n- **Type**: document\n- **Tags**: document, All_of_it_Anything_Everything_At_Once, bug\n- **Imported**: 2025-05-05T16:25:37.153Z",
  "source": {
    "type": "notebook",
    "path": "All of it Anything Everything At Once/old_mic_vad_streaming.txt.txt",
    "importedAt": "2025-05-05T16:25:37.153Z"
  },
  "created": "2025-05-05T16:25:37.153Z",
  "modified": "2025-05-05T16:25:37.153Z",
  "tags": [
    "document",
    "All_of_it_Anything_Everything_At_Once",
    "bug"
  ]
}