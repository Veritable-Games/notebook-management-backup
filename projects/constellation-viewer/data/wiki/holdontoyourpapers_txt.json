{
  "title": "holdontoyourpapers_txt",
  "content": "# holdontoyourpapers.txt\n\nContent-Type: text/x-zim-wiki\nWiki-Format: zim 0.6\nCreation-Date: 2024-07-23T15:11:22-07:00\n\n====== holdontoyourpapers.txt ======\n4:03 PM]Glitch: https://arxiv.org/abs/2303.03103\r\narXiv.org\r\nTowards Zero-Shot Functional Compositionality of Language Models\r\nLarge Pre-trained Language Models (PLM) have become the most desirable\r\nstarting point in the field of NLP, as they have become remarkably good at\r\nsolving many individual tasks. Despite such...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2303.03004\r\narXiv.org\r\nxCodeEval: A Large Scale Multilingual Multitask Benchmark for Code...\r\nThe ability to solve problems is a hallmark of intelligence and has been an\r\nenduring goal in AI. AI systems that can create programs as solutions to\r\nproblems or assist developers in writing...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2302.04761\r\narXiv.org\r\nToolformer: Language Models Can Teach Themselves to Use Tools\r\nLanguage models (LMs) exhibit remarkable abilities to solve new tasks from\r\njust a few examples or textual instructions, especially at scale. They also,\r\nparadoxically, struggle with basic...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2303.00001\r\narXiv.org\r\nReward Design with Language Models\r\nReward design in reinforcement learning (RL) is challenging since specifying\r\nhuman notions of desired behavior may be difficult via reward functions or\r\nrequire many expert demonstrations. Can we...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2302.06706\r\narXiv.org\r\nOn the Planning Abilities of Large Language Models (A Critical...\r\nIntrigued by the claims of emergent reasoning capabilities in LLMs trained on\r\ngeneral web corpora, in this paper, we set out to investigate their planning\r\ncapabilities. We aim to evaluate (1) how...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2207.05987\r\narXiv.org\r\nDocPrompting: Generating Code by Retrieving the Docs\r\nPublicly available source-code libraries are continuously growing and\r\nchanging. This makes it impossible for models of code to keep current with all\r\navailable APIs by simply training these models...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2302.14045\r\narXiv.org\r\nLanguage Is Not All You Need: Aligning Perception with Language Models\r\nA big convergence of language, multimodal perception, action, and world\r\nmodeling is a key step toward artificial general intelligence. In this work, we\r\nintroduce Kosmos-1, a Multimodal Large...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2302.11466\r\narXiv.org\r\nAdvancements in Federated Learning: Models, Methods, and Privacy\r\nFederated learning (FL) is a promising technique for addressing the rising\r\nprivacy and security issues. Its main ingredient is to cooperatively learn the\r\nmodel among the distributed clients...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2212.09689\r\narXiv.org\r\nUnnatural Instructions: Tuning Language Models with (Almost) No Hum...\r\nInstruction tuning enables pretrained language models to perform new tasks\r\nfrom inference-time natural language descriptions. These approaches rely on\r\nvast amounts of human supervision in the form...\r\nImage\r\n[4:03 PM]Glitch: https://arxiv.org/abs/2210.05359\r\narXiv.org\r\nMind's Eye: Grounded Language Model Reasoning through Simulation\r\nSuccessful and effective communication between humans and AI relies on a\r\nshared experience of the world. By training solely on written text, current\r\nlanguage models (LMs) miss the grounded...\r\nImage\r\n[5:13 PM]Glitch: https://arxiv.org/abs/2303.05398\r\narXiv.org\r\nMathPrompter: Mathematical Reasoning using Large Language Models\r\nLarge Language Models (LLMs) have limited performance when solving arithmetic\r\nreasoning tasks and often provide incorrect answers. Unlike natural language\r\nunderstanding, math problems typically...\r\nImage\r\n[5:13 PM]Glitch: https://arxiv.org/abs/2303.05510\r\narXiv.org\r\nPlanning with Large Language Models for Code Generation\r\nExisting large language model-based code generation pipelines typically use\r\nbeam search or sampling algorithms during the decoding process. Although the\r\nprograms they generate achieve high token-matching-based scores, they often\r\nfail to compile or generate incorrect outputs. The main reason is that\r\nconventional Transformer decoding algorithms ma...\r\nImage\r\n[5:13 PM]Glitch: https://ai.googleblog.com/2022/11/better-language-models-without-massive.html?m=1\r\nBetter Language Models Without Massive Compute\r\nBetter Language Models Without Massive Compute\r\n[5:17 PM]Glitch: https://www.nature.com/articles/s41562-022-01516-2\r\nNature\r\nEvidence of a predictive coding hierarchy in the human brain listen...\r\nNature Human Behaviour - Current machine learning language algorithms make adjacent word-level predictions. In this work, Caucheteux et al. show that the human brain probably uses long-range and...\r\n[5:29 PM]Glitch: https://arxiv.org/abs/2302.06675\r\nhttps://arxiv.org/abs/2302.07080\r\nhttps://arxiv.org/abs/2212.14034\r\nhttps://arxiv.org/abs/2302.00923\r\narXiv.org\r\nSymbolic Discovery of Optimization Algorithms\r\nWe present a method to formulate algorithm discovery as program search, and\r\napply it to discover optimization algorithms for deep neural network training.\r\nWe leverage efficient search techniques to explore an infinite and sparse\r\nprogram space. To bridge the large generalization gap between proxy and target\r\ntasks, we also introduce program select...\r\nImage\r\narXiv.org\r\nThe Programmer's Assistant: Conversational Interaction with a Large...\r\nLarge language models (LLMs) have recently been applied in software\r\nengineering to perform tasks such as translating code between programming\r\nlanguages, generating code from natural language, and autocompleting code as it\r\nis being written. When used within development tools, these systems typically\r\ntreat each model invocation independently from ...\r\nImage\r\narXiv.org\r\nCramming: Training a Language Model on a Single GPU in One Day\r\nRecent trends in language modeling have focused on increasing performance\r\nthrough scaling, and have resulted in an environment where training language\r\nmodels is out of reach for most researchers and practitioners. While most in\r\nthe community are asking how to push the limits of extreme computation, we ask\r\nthe opposite question: How far can we ge...\r\nImage\r\narXiv.org\r\nMultimodal Chain-of-Thought Reasoning in Language Models\r\nLarge language models (LLMs) have shown impressive performance on complex\r\nreasoning by leveraging chain-of-thought (CoT) prompting to generate\r\nintermediate reasoning chains as the rationale to infer the answer. However,\r\nexisting CoT studies have focused on the language modality. We propose\r\nMultimodal-CoT that incorporates language (text) and vis...\r\nImage\r\n[5:30 PM]Glitch: https://arxiv.org/abs/2302.08399\r\narXiv.org\r\nLarge Language Models Fail on Trivial Alterations to Theory-of-Mind...\r\nIntuitive psychology is a pillar of common-sense reasoning. The replication\r\nof this reasoning in machine intelligence is an important stepping-stone on the\r\nway to human-like artificial intelligence. Several recent tasks and benchmarks\r\nfor examining this reasoning in Large-Large Models have focused in particular\r\non belief attribution in Theory-of...\r\nImage\r\n[5:40 PM]Glitch: https://arxiv.org/abs/2303.05511\r\narXiv.org\r\nScaling up GANs for Text-to-Image Synthesis\r\nThe recent success of text-to-image synthesis has taken the world by storm\r\nand captured the general public's imagination. From a technical standpoint, it\r\nalso marked a drastic change in the favored architecture to design generative\r\nimage models. GANs used to be the de facto choice, with techniques like\r\nStyleGAN. With DALL-E 2, auto-regressive an...\r\nImage\r\n[5:41 PM]Glitch: https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1\r\nPapers with Code - LLaMA: Open and Efficient Foundation Language Mo...\r\nüèÜ SOTA for Question Answering on PIQA (Accuracy metric)\r\nPapers with Code - LLaMA: Open and Efficient Foundation Language Mo...\r\nGlitch\r\n pinned \r\na message\r\n to this channel. See all \r\npinned messages\r\n.\r\n ‚Äî 03/11/2023 5:41 PM\r\nGlitch\r\n pinned \r\na message\r\n to this channel. See all \r\npinned messages\r\n.\r\n ‚Äî 03/11/2023 9:23 PM\r\n[6:10 PM]Glitch: https://arxiv.org/abs/2201.08808 \r\narXiv.org\r\nConversational Information Seeking\r\nConversational information seeking (CIS) is concerned with a sequence of\r\ninteractions between one or more users and an information system. Interactions\r\nin CIS are primarily based on natural language dialogue, while they may include\r\nother types of interactions, such as click, touch, and body gestures. This\r\nmonograph provides a thorough overview o...\r\nImage\r\n[6:11 PM]Glitch: https://link.springer.com/chapter/10.1007/978-3-642-32090-3_21\r\nSpringerLink\r\nAn Engineering Approach to Conversational Informatics\r\nConversational informatics is a field of research that focuses on investigating human behaviors and designing artifacts that can interact with people in a conversational fashion. The field draws on a foundation provided by artificial intelligence, natural language...\r\nImage\r\n[6:37 PM]Glitch: this would be the published textbook - I have a physical copy: might transcribe it for reference.\r\nhttps://onlinelibrary.wiley.com/doi/book/10.1002/9780470512470\r\n[7:02 PM]Glitch: https://arxiv.org/abs/2301.04589\r\narXiv.org\r\nMemory Augmented Large Language Models are Computationally Universal\r\nWe show that transformer-based large language models are computationally\r\nuniversal when augmented with an external memory. Any deterministic language\r\nmodel that conditions on strings of bounded length is equivalent to a finite\r\nautomaton, hence computationally limited. However, augmenting such models with\r\na read-write memory creates the possibili...\r\nImage\r\n[11:11 AM]Glitch: \"Prompting Large Language Models\r\nWith the Socratic Method\"\r\nhttps://arxiv.org/pdf/2303.08769.pdf \r\n[3:47 PM]Glitch: 'Cetacean Translation Initiative: a roadmap to deciphering the\r\ncommunication of sperm whales'\r\nhttps://arxiv.org/ftp/arxiv/papers/2104/2104.08614.pdf\r\n[3:48 PM]Glitch: 'Sparks of Artificial General Intelligence:\r\nEarly experiments with GPT-4'\r\nhttps://arxiv.org/pdf/2303.12712.pdf\r\n[4:56 PM]Glitch: https://github.com/daveshap/Functional_Sentience/blob/main/transcript.md\r\nGitHub\r\nFunctional_Sentience/transcript.md at main ¬∑ daveshap/Functional_Se...\r\nPaper on Functional (vs Philosophical) Sentience. Contribute to daveshap/Functional_Sentience development by creating an account on GitHub.\r\nFunctional_Sentience/transcript.md at main ¬∑ daveshap/Functional_Se...\r\n[10:15 AM]Glitch: https://aiindex.stanford.edu/report/\r\nArtificial Intelligence Index\r\ndigitalavenues\r\nAI Index Report 2023\r\n[11:39 PM]Glitch: https://arxiv.org/abs/2212.01354\r\narXiv.org\r\nDesigning Ecosystems of Intelligence from First Principles\r\nThis white paper lays out a vision of research and development in the field\r\nof artificial intelligence for the next decade (and beyond). Its denouement is\r\na cyber-physical ecosystem of natural and synthetic sense-making, in which\r\nhumans are integral participants$\\unicode{x2014}$what we call ''shared\r\nintelligence''. This vision is premised on act...\r\nImage\r\n[1:20 AM]Glitch: https://arxiv.org/abs/2004.09473\r\narXiv.org\r\nAttention Routing: track-assignment detailed routing using attentio...\r\nIn the physical design of integrated circuits, global and detailed routing\r\nare critical stages involving the determination of the interconnected paths of\r\neach net on a circuit while satisfying the design constraints. Existing actual\r\nrouters as well as routability predictors either have to resort to expensive\r\napproaches that lead to high computat...\r\nImage\r\n[12:47 PM]Glitch: https://www.biorxiv.org/content/10.1101/2022.11.18.517004v2.full.pdf\r\n[12:47 PM]Glitch: \"experiment where they were able to reconstruct a visual image from an fMRI scan using Stable Diffusion\"\r\n[6:12 PM]Glitch: @Teradimer\r\n[6:13 PM]Teradimer: mhm\r\n[2:42 PM]Glitch: https://arxiv.org/abs/2303.11366\r\narXiv.org\r\nReflexion: an autonomous agent with dynamic memory and self-reflection\r\nRecent advancements in decision-making large language model (LLM) agents have\r\ndemonstrated impressive performance across various benchmarks. However, these\r\nstate-of-the-art approaches typically necessitate internal model fine-tuning,\r\nexternal model fine-tuning, or policy optimization over a defined state space.\r\nImplementing these methods can pro...\r\nImage\r\n[8:12 PM]Glitch: https://arxiv.org/abs/2210.03629#google\r\narXiv.org\r\nReAct: Synergizing Reasoning and Acting in Language Models\r\nWhile large language models (LLMs) have demonstrated impressive capabilities\r\nacross tasks in language understanding and interactive decision making, their\r\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\r\naction plan generation) have primarily been studied as separate topics. In this\r\npaper, we explore the use of LLMs to ...\r\nImage\r\n[11:25 PM]Glitch: https://arxiv.org/abs/2212.14704\r\narXiv.org\r\nDream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Te...\r\nRecent CLIP-guided 3D optimization methods, such as DreamFields and\r\nPureCLIPNeRF, have achieved impressive results in zero-shot text-to-3D\r\nsynthesis. However, due to scratch training and random initialization without\r\nprior knowledge, these methods often fail to generate accurate and faithful 3D\r\nstructures that conform to the input text. In this ...\r\nImage\r\n[11:25 PM]Glitch: https://arxiv.org/abs/2211.10440\r\narXiv.org\r\nMagic3D: High-Resolution Text-to-3D Content Creation\r\nDreamFusion has recently demonstrated the utility of a pre-trained\r\ntext-to-image diffusion model to optimize Neural Radiance Fields (NeRF),\r\nachieving remarkable text-to-3D synthesis results. However, the method has two\r\ninherent limitations: (a) extremely slow optimization of NeRF and (b)\r\nlow-resolution image space supervision on NeRF, leading to...\n\n\n## Metadata\n- **Source**: All of it Anything Everything At Once/holdontoyourpapers.txt.txt\n- **Type**: document\n- **Tags**: document, All_of_it_Anything_Everything_At_Once, dialogue, quest\n- **Imported**: 2025-05-05T16:25:37.140Z",
  "source": {
    "type": "notebook",
    "path": "All of it Anything Everything At Once/holdontoyourpapers.txt.txt",
    "importedAt": "2025-05-05T16:25:37.140Z"
  },
  "created": "2025-05-05T16:25:37.140Z",
  "modified": "2025-05-05T16:25:37.140Z",
  "tags": [
    "document",
    "All_of_it_Anything_Everything_At_Once",
    "dialogue",
    "quest"
  ]
}